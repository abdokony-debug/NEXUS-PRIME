import os
import sys
import re
import json
import time
import random
import requests
import smtplib
from datetime import datetime
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

# Libraries
from supabase import create_client
from groq import Groq
from duckduckgo_search import DDGS
from fake_useragent import UserAgent
from loguru import logger

# --- SETUP LOGGING ---
logger.remove()
logger.add(sys.stderr, format="<green>{time:HH:mm:ss}</green> | <level>{message}</level>", level="INFO")

# --- CONFIGURATION ---
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
EMAIL_USER = os.getenv("EMAIL_USER")
EMAIL_PASS = os.getenv("EMAIL_PASS")

if not SUPABASE_URL or not SUPABASE_KEY:
    logger.critical("‚ùå FATAL: Database credentials missing.")
    sys.exit(1)

# --- CORE CLASSES ---

class StealthNetwork:
    """ŸÖÿ≥ÿ§ŸàŸÑ ÿπŸÜ ÿßŸÑÿ™ÿÆŸÅŸä Ÿàÿ™ÿ∫ŸäŸäÿ± ÿßŸÑŸáŸàŸäÿ© ÿßŸÑÿ±ŸÇŸÖŸäÿ©"""
    def __init__(self):
        self.ua = UserAgent()
        self.proxies = [] # ŸäŸÖŸÉŸÜ ÿ™ŸÅÿπŸäŸÑ ÿ¨ŸÑÿ® ÿßŸÑÿ®ÿ±ŸàŸÉÿ≥Ÿäÿßÿ™ ŸáŸÜÿß
        
    def get_headers(self):
        """ÿ™ŸàŸÑŸäÿØ ŸáŸàŸäÿ© ŸÖÿ™ÿµŸÅÿ≠ ÿ¨ÿØŸäÿØÿ© ŸÅŸä ŸÉŸÑ ÿ∑ŸÑÿ®"""
        return {
            "User-Agent": self.ua.random,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "Connection": "keep-alive",
            "Referer": "https://www.google.com/"
        }

class MultiEngineSearch:
    """ŸÖÿ≠ÿ±ŸÉ ÿ®ÿ≠ÿ´ ŸÖÿ™ÿπÿØÿØ ÿßŸÑÿ±ÿ§Ÿàÿ≥ (ŸáŸäÿØÿ±ÿß)"""
    def __init__(self):
        self.ddgs = DDGS()
        self.network = StealthNetwork()
    
    def search_duckduckgo(self, query):
        """ÿßŸÑŸÖÿ≠ÿßŸàŸÑÿ© ÿßŸÑÿ£ŸàŸÑŸâ: DuckDuckGo"""
        try:
            # backend='lite' ŸáŸà ÿßŸÑÿ£ÿ≥ÿ±ÿπ ŸàÿßŸÑÿ£ŸÇŸÑ ÿ≠ÿ∏ÿ±ÿßŸã
            results = self.ddgs.text(query, max_results=8, backend='lite')
            if results: return results
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è DDG Failed: {e}")
        return []

    def search_google_fallback(self, query):
        """ÿßŸÑŸÖÿ≠ÿßŸàŸÑÿ© ÿßŸÑÿ´ÿßŸÜŸäÿ©: ŸÖÿ≠ÿßŸÉÿßÿ© ÿ®ÿ≠ÿ´ ÿ¨Ÿàÿ¨ŸÑ"""
        # ŸÖŸÑÿßÿ≠ÿ∏ÿ©: Ÿáÿ∞ÿß ÿßŸÑÿ≥ŸÉÿ±Ÿäÿ®ÿ™ Ÿäÿ≠ÿßŸàŸÑ ŸÇÿ±ÿßÿ°ÿ© ÿ¨Ÿàÿ¨ŸÑ ŸÉŸÖÿ™ÿµŸÅÿ≠
        # ŸÅŸä ÿ®Ÿäÿ¶ÿ© GitHub ŸÇÿØ ŸäŸÉŸàŸÜ ÿµÿπÿ®ÿßŸãÿå ŸÑŸÉŸÜŸÜÿß ŸÜÿ∂ÿπŸá ŸÉÿßÿ≠ÿ™Ÿäÿßÿ∑Ÿä
        try:
            logger.info("üîÑ Switching to Google Scraping Mode...")
            headers = self.network.get_headers()
            params = {'q': query, 'num': 10}
            response = requests.get('https://www.google.com/search', headers=headers, params=params, timeout=10)
            
            if response.status_code == 200:
                # ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿ®ÿ≥Ÿäÿ∑ ÿ¨ÿØÿßŸã (Regex) ŸÑÿ™ŸÅÿßÿØŸä ÿ™ÿπŸÇŸäÿØ HTML
                # Ÿáÿ∞Ÿá ÿ∑ÿ±ŸäŸÇÿ© 'ŸÇÿ∞ÿ±ÿ©' ŸÑŸÉŸÜŸáÿß ŸÅÿπÿßŸÑÿ© ŸÑŸÑÿ∑Ÿàÿßÿ±ÿ¶
                links = re.findall(r'href="/url\?q=(https://[^&]+)', response.text)
                clean_results = []
                for link in links[:5]:
                    if "google" not in link:
                        clean_results.append({'title': 'Google Result', 'body': 'Found via Google Fallback', 'href': link})
                return clean_results
        except Exception as e:
            logger.error(f"‚ùå Google Fallback Failed: {e}")
        return []

    def execute_search(self, query):
        """ÿßŸÑŸÖÿØŸäÿ± ÿßŸÑÿ∞Ÿä ŸäŸÇÿ±ÿ± ÿ£Ÿä ŸÖÿ≠ÿ±ŸÉ Ÿäÿ≥ÿ™ÿÆÿØŸÖ"""
        # 1. ÿ¨ÿ±ÿ® DuckDuckGo
        results = self.search_duckduckgo(query)
        if results: return results
        
        # 2. ÿßŸÜÿ™ÿ∏ÿ± ŸÇŸÑŸäŸÑÿßŸã Ÿàÿ¨ÿ±ÿ® ÿ¨Ÿàÿ¨ŸÑ
        time.sleep(random.uniform(2, 5))
        return self.search_google_fallback(query)

class NeuralBrain:
    def __init__(self):
        self.client = Groq(api_key=GROQ_API_KEY)

    def analyze(self, content, campaign):
        prompt = f"""
        Role: Marketing Sniper.
        Product: {campaign['product_link']}
        USP: {campaign['usp']}
        Content: "{content[:1000]}"
        
        Task:
        1. Is this relevant? (True/False)
        2. Score Intent (0-100).
        3. Draft Email.
        
        Return JSON: {{ "score": int, "is_relevant": bool, "subject": "str", "body": "str" }}
        """
        try:
            completion = self.client.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                model="llama3-70b-8192",
                response_format={"type": "json_object"}
            )
            return json.loads(completion.choices[0].message.content)
        except:
            return {"score": 0, "is_relevant": False}

class NexusHydra:
    def __init__(self):
        self.supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
        self.search_engine = MultiEngineSearch()
        self.brain = NeuralBrain()

    def run(self):
        logger.info("üêâ NEXUS-HYDRA: ACTIVATED. ADAPTIVE MODE ON.")
        
        try:
            campaigns = self.supabase.table('campaigns').select("*").eq('status', 'active').execute().data
        except Exception as e:
            logger.error(f"DB Connection Failed: {e}")
            return

        if not campaigns:
            logger.warning("No active missions.")
            return

        for camp in campaigns:
            quota = camp.get('max_leads') or 5
            acquired = 0
            
            # ÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑŸÉŸÑŸÖÿßÿ™
            keywords = [k.strip() for k in camp['keywords'].replace('"', '').split(',')]
            
            logger.info(f"‚öîÔ∏è Mission: {camp['name']} | Targets: {keywords}")

            for kw in keywords:
                if acquired >= quota: break

                # ÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿßÿ™ ÿ∞ŸÉŸäÿ© ŸÖÿ™ÿ∫Ÿäÿ±ÿ©
                queries = [
                    f'{kw} site:reddit.com',         # ÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ© 1: ÿßŸÑŸÖŸÜÿ™ÿØŸäÿßÿ™
                    f'{kw} "looking for"',           # ÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ© 2: ÿßŸÑŸÜŸäÿ© ÿßŸÑŸÖÿ®ÿßÿ¥ÿ±ÿ©
                    f'best {kw} 2025',               # ÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ© 3: ÿßŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿπÿßŸÖ
                ]

                for q in queries:
                    if acquired >= quota: break
                    
                    logger.info(f"üîé Hunting: {q}")
                    results = self.search_engine.execute_search(q)
                    
                    if not results:
                        logger.warning("   -> No signals. Adapting...")
                        continue
                    
                    logger.info(f"   -> Found {len(results)} signals. Analyzing...")
                    
                    for res in results:
                        if acquired >= quota: break
                        
                        # ÿØŸÖÿ¨ ÿßŸÑÿπŸÜŸàÿßŸÜ ŸÖÿπ ÿßŸÑŸàÿµŸÅ ŸÑŸÑÿ™ÿ≠ŸÑŸäŸÑ
                        content = f"{res.get('title', '')} {res.get('body', '')}"
                        analysis = self.brain.analyze(content, camp)
                        
                        if analysis.get('score', 0) > 75:
                            logger.success(f"   üéØ TARGET LOCKED (Score: {analysis['score']})")
                            
                            # ÿßŸÑÿ≠ŸÅÿ∏
                            self.supabase.table('leads').upsert({
                                "campaign_id": camp['id'],
                                "url": res['href'],
                                "intent_score": analysis['score'],
                                "ai_analysis": str(analysis),
                                "message_draft": analysis.get('body'),
                                "status": "ready",
                                "created_at": datetime.utcnow().isoformat()
                            }, on_conflict='url').execute()
                            
                            acquired += 1
                        
                    time.sleep(random.uniform(1, 3)) # ÿßÿ≥ÿ™ÿ±ÿßÿ≠ÿ© ÿ®ÿ¥ÿ±Ÿäÿ©

            if acquired > 0:
                logger.success(f"‚úÖ Campaign {camp['name']} finished with {acquired} leads.")
            else:
                logger.error(f"‚ùå Campaign {camp['name']} failed to find targets. Try broader keywords.")

if __name__ == "__main__":
    NexusHydra().run()
