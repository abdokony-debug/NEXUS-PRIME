# nexus_prime.py
import os
import sys
import json
import uuid
import asyncio
import logging
import aiohttp
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from urllib.parse import urlparse
from supabase import create_client
import re

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('nexus_prime.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Environment
SUPABASE_URL = os.getenv('SUPABASE_URL', '')
SUPABASE_KEY = os.getenv('SUPABASE_KEY', '')
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY', '')
GOOGLE_CX = os.getenv('GOOGLE_CX', '')

# Models
class Platform:
    TWITTER = "twitter"
    LINKEDIN = "linkedin"
    EMAIL = "email"
    GITHUB = "github"
    GENERIC = "generic"

class LeadStatus:
    NEW = "new"
    CONTACTED = "contacted"
    CONVERTED = "converted"

class CampaignStatus:
    ACTIVE = "active"
    PAUSED = "paused"
    COMPLETED = "completed"

class Lead:
    def __init__(self):
        self.id = str(uuid.uuid4())
        self.campaign_id = ""
        self.url = ""
        self.platform = Platform.GENERIC
        self.name = ""
        self.email = ""
        self.company = ""
        self.intent_score = 0.0
        self.content_summary = ""
        self.status = LeadStatus.NEW
        self.message_sent = False
        self.created_at = datetime.now()

class Campaign:
    def __init__(self):
        self.id = ""
        self.name = ""
        self.keywords = []
        self.usp = ""
        self.product_link = ""
        self.max_leads = 100
        self.min_intent_score = 70.0
        self.status = CampaignStatus.ACTIVE

    @classmethod
    def from_db(cls, data: Dict):
        campaign = cls()
        campaign.id = data.get('id', '')
        campaign.name = data.get('name', '')
        campaign.keywords = [k.strip() for k in data.get('keywords', '').split(',') if k.strip()]
        campaign.usp = data.get('usp', '')
        campaign.product_link = data.get('product_link', '')
        campaign.max_leads = data.get('max_leads', 100)
        campaign.min_intent_score = data.get('min_intent_score', 70.0)
        campaign.status = data.get('status', CampaignStatus.ACTIVE)
        return campaign

# Database
class DatabaseService:
    def __init__(self):
        self.client = create_client(SUPABASE_URL, SUPABASE_KEY)
    
    def get_active_campaigns(self) -> List[Campaign]:
        try:
            res = self.client.table('campaigns').select('*').eq('status', 'active').execute()
            return [Campaign.from_db(row) for row in res.data]
        except Exception as e:
            logger.error(f"Database error: {e}")
            return []
    
    def insert_lead(self, lead: Lead) -> bool:
        try:
            data = {
                'id': lead.id,
                'campaign_id': lead.campaign_id,
                'url': lead.url,
                'platform': lead.platform,
                'name': lead.name,
                'email': lead.email,
                'company': lead.company,
                'intent_score': lead.intent_score,
                'content_summary': lead.content_summary[:500],
                'status': lead.status,
                'message_sent': lead.message_sent,
                'created_at': lead.created_at.isoformat()
            }
            self.client.table('leads').insert(data).execute()
            return True
        except Exception as e:
            logger.error(f"Insert lead failed: {e}")
            return False
    
    def update_campaign_status(self, campaign_id: str, status: str):
        try:
            self.client.table('campaigns').update({'status': status}).eq('id', campaign_id).execute()
        except Exception as e:
            logger.error(f"Update campaign failed: {e}")

# Intent Analyzer
class IntentAnalyzer:
    def __init__(self):
        try:
            from transformers import pipeline
            self.classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")
        except:
            self.classifier = None
    
    def score(self, text: str) -> float:
        if not text or len(text) < 10:
            return 0.0
        
        if self.classifier:
            return self._score_with_transformers(text)
        return self._score_fallback(text)
    
    def _score_with_transformers(self, text: str) -> float:
        try:
            labels = ["buying intent", "researching", "complaining", "casual"]
            result = self.classifier(text[:500], labels)
            top_score = result['scores'][0] * 100
            
            if result['labels'][0] == "buying intent":
                return top_score
            elif result['labels'][0] == "researching":
                return top_score * 0.7
            else:
                return top_score * 0.3
        except:
            return self._score_fallback(text)
    
    def _score_fallback(self, text: str) -> float:
        keywords = {
            'buy': 90, 'purchase': 85, 'need': 80, 'looking for': 75,
            'problem': 70, 'solution': 65, 'review': 60, 'compare': 55
        }
        
        text_lower = text.lower()
        max_score = 0
        
        for keyword, score in keywords.items():
            if keyword in text_lower:
                max_score = max(max_score, score)
        
        if max_score == 0:
            max_score = 40 if len(text) > 100 else 20
        
        return float(max_score)

# Lead Finder
class LeadFinder:
    def __init__(self):
        self.session = None
    
    async def search(self, query: str, num_results: int = 10) -> List[str]:
        if not GOOGLE_API_KEY or not GOOGLE_CX:
            return self._fallback_search(query, num_results)
        
        try:
            async with aiohttp.ClientSession() as session:
                url = "https://www.googleapis.com/customsearch/v1"
                params = {
                    'q': query,
                    'key': GOOGLE_API_KEY,
                    'cx': GOOGLE_CX,
                    'num': min(num_results, 10)
                }
                
                async with session.get(url, params=params) as response:
                    data = await response.json()
                    return [item['link'] for item in data.get('items', [])]
        except Exception as e:
            logger.error(f"Search error: {e}")
            return self._fallback_search(query, num_results)
    
    def _fallback_search(self, query: str, num_results: int) -> List[str]:
        platforms = ['twitter.com', 'linkedin.com', 'github.com']
        return [f"https://{platform}/search?q={query.replace(' ', '+')}" for platform in platforms[:num_results]]

# Content Scraper
class ContentScraper:
    def __init__(self):
        self.session = None
    
    async def scrape(self, url: str) -> Dict:
        try:
            async with aiohttp.ClientSession() as session:
                headers = {'User-Agent': 'Mozilla/5.0'}
                async with session.get(url, headers=headers, timeout=10) as response:
                    html = await response.text()
                    return self._extract_data(html, url)
        except Exception as e:
            logger.error(f"Scrape error for {url}: {e}")
            return {'content': '', 'platform': Platform.GENERIC}
    
    def _extract_data(self, html: str, url: str) -> Dict:
        import re
        
        data = {'content': '', 'platform': Platform.GENERIC}
        
        # Extract platform
        url_lower = url.lower()
        for platform in [Platform.TWITTER, Platform.LINKEDIN, Platform.GITHUB]:
            if platform in url_lower:
                data['platform'] = platform
                break
        
        # Extract content (simple regex)
        text = re.sub(r'<[^>]+>', ' ', html)
        text = re.sub(r'\s+', ' ', text)
        data['content'] = text[:2000]
        
        # Extract email
        emails = re.findall(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', html)
        if emails:
            data['email'] = emails[0]
        
        # Extract name from URL
        parsed = urlparse(url)
        path_parts = parsed.path.strip('/').split('/')
        if path_parts:
            data['name'] = path_parts[0].replace('-', ' ').title()
        
        return data

# Messenger
class Messenger:
    async def send(self, lead: Lead, campaign: Campaign) -> bool:
        logger.info(f"Would send message to {lead.url}")
        lead.message_sent = True
        lead.status = LeadStatus.CONTACTED
        return True

# Rate Limiter
class RateLimiter:
    def __init__(self):
        self.sent_messages = {}
    
    def can_send(self, campaign_id: str) -> bool:
        today = datetime.now().date().isoformat()
        key = f"{campaign_id}_{today}"
        
        count = self.sent_messages.get(key, 0)
        if count >= 50:  # Max 50 per day per campaign
            return False
        
        self.sent_messages[key] = count + 1
        return True

# Main System
class NexusPrime:
    def __init__(self):
        self.db = DatabaseService()
        self.analyzer = IntentAnalyzer()
        self.finder = LeadFinder()
        self.scraper = ContentScraper()
        self.messenger = Messenger()
        self.limiter = RateLimiter()
        self.running = False
    
    async def run(self):
        self.running = True
        logger.info("NEXUS-PRIME starting...")
        
        while self.running:
            try:
                await self._process_campaigns()
                await asyncio.sleep(300)  # 5 minutes
            except Exception as e:
                logger.error(f"Main loop error: {e}")
                await asyncio.sleep(60)
    
    async def _process_campaigns(self):
        campaigns = self.db.get_active_campaigns()
        
        for campaign in campaigns:
            if not self.running:
                break
            
            logger.info(f"Processing campaign: {campaign.name}")
            await self._process_campaign(campaign)
            await asyncio.sleep(2)
    
    async def _process_campaign(self, campaign: Campaign):
        query = " ".join(campaign.keywords)
        urls = await self.finder.search(query, campaign.max_leads)
        
        for url in urls[:campaign.max_leads]:
            if not self.running:
                break
            
            try:
                # Scrape content
                scraped = await self.scraper.scrape(url)
                
                # Analyze intent
                intent_score = self.analyzer.score(scraped['content'])
                
                if intent_score >= campaign.min_intent_score:
                    # Create lead
                    lead = Lead()
                    lead.campaign_id = campaign.id
                    lead.url = url
                    lead.platform = scraped['platform']
                    lead.name = scraped.get('name', '')
                    lead.email = scraped.get('email', '')
                    lead.intent_score = intent_score
                    lead.content_summary = scraped['content'][:500]
                    
                    # Save lead
                    if self.db.insert_lead(lead):
                        logger.info(f"Lead saved: {url} (Score: {intent_score})")
                        
                        # Send message if rate limit allows
                        if self.limiter.can_send(campaign.id):
                            await self.messenger.send(lead, campaign)
                            self.db.insert_lead(lead)
                
                await asyncio.sleep(1)
                
            except Exception as e:
                logger.error(f"Error processing {url}: {e}")
                await asyncio.sleep(2)
        
        # Check if campaign reached max leads
        stats = self.db.get_campaign_stats(campaign.id)
        if stats['total_leads'] >= campaign.max_leads:
            self.db.update_campaign_status(campaign.id, CampaignStatus.COMPLETED)
            logger.info(f"Campaign {campaign.name} completed")

    def stop(self):
        self.running = False
        logger.info("NEXUS-PRIME stopping...")

# Startup
if __name__ == "__main__":
    # Check environment
    required = ['SUPABASE_URL', 'SUPABASE_KEY']
    missing = [var for var in required if not os.getenv(var)]
    
    if missing:
        logger.error(f"Missing environment variables: {missing}")
        sys.exit(1)
    
    # Run system
    system = NexusPrime()
    
    try:
        asyncio.run(system.run())
    except KeyboardInterrupt:
        system.stop()
    except Exception as e:
        logger.error(f"System error: {e}")
        sys.exit(1)
